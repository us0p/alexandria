## LangGraph
Can enhance tool use even further by structuring tasks in workflows called graphs.

In this graphs, tasks called "nodes" are connected by rules called "edges".

![[Pasted image 20250725182012.png]]

For example, a database query node can link to a document retrieval node, with an edge pointing to which document is retrieved.
## Graph State
Organizes the agent's order of tasks, such as tool usage and LLM calls, into a workflow.
## Agent State
Used by the graph to track the agent's progress as text to help determine when a task is complete.
## Nodes
Represents functions, like generating a response or calling a tool.
## Edges
Define connections between these nodes.

In the picture bellow, an edge connects the start of the workflow to the chat bot node and another edge connects the chat bot node to the end of the workflow.

![[Pasted image 20250726100035.png]]
## Pre-built nodes
`START` and `END` are pre-build nodes that represents the start and the end of a workflow.

```python
from typing import Annotated
from typing_extensions import TypedDict

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(...)

# Agent called state that uses TypedDict to structure messages.
class State(TypedDict):
	# List that will store all text interactions with the chat bot.
	# Annotated with add_messages, ensures that new messages are added to this list with metadata
	messages: Annotated[list, add_messages]

# Organizes the nodes and edges in the chatbot's workflow
graph_builder = StateGraph(State)

# main function
def chatbot(state: State):
	# Uses the state messages to generate a response based on the conversation so far.
	return {"messages": [llm.invoke(state["messages"])]}

# Adds chat bot as a node to the graph
graph_builder.add_node("chatbot", chatbot)

# Add edges connecting the start and end of the workflow to the node.
graph_builder.add_edge(START, "chatbot")
graph_builder.add_edge("chatbot", END)

# Compile the graph as an application
graph = graph_builder.compile()
```
## Streaming graph events
Allow us to process the steps in an agent's workflow in real time.

As we stream, each event represents a single step in the graph, like generating a response or calling a tool.

This let's us track the chat bot agent's progress by displaying responses as soon as they're ready.
```python
# execute the chatbot based on user input
def stream_graph_updates(user_input: str):
	# Start streaming events from the graph with the user's input
	# User input is labeled as "user" and appended to the graph messages list
	for event in graph.stream({"messages": [("user", user_input)]}):
		# We loop by each event generated by the graph, retrieving the last chatbot response with
		for value in event.values():
			# Displays chatbot response as soons as it's ready.
			print("Agent": value["messages"])

user_query = "Who is Mary Shelley?"
stream_graph_updates(user_query)
```
## Adding external tools
```python
# Allows interaction with Wikipedia API
from langchain_community.utilities import WikipediaAPIWrapper
# Makes the API a tool for running queries
from langchain_community.tools import WikipediaQueryRun
# Modules for adding tool conditions and nodes
from langgraph.prebuilt import ToolNode, tools_condition

# Setting top_k_results to 1 to keep responses relevants.
api_wrapper = WikipediaAPIWrapper(top_k_results=1)

# Creates a wikipedia query tool using the API wrapper.
wikipedia_tool = WikipediaQueryRun(api_wrapper=api_wrapper)

tools = [wikipedia_tool]

# Binding the tools to the language model using .bind_tools()
llm_with_tools = llm.bind_tools(tools)

# Updates original chatbot function to use the new tools
def chatbot(state: State):
	return {"messages": [llm_with_tools.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)

# Create a ToolNode to handle tool calls and add it to the graph
tool_node = ToolNode(tools=[wikipedia_tool])
graph_builder.add_node("tools", tool_node)

# Set up a condition to direct from chatbot to tool or end node
# Allows the chatbot to decide if the tool is needed
graph_builder.add_conditional_edges("chatbot", tools_condition)

# If the tools node is executed we need to link it back to our chatbot so it can continue to the end of the flow
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge(START, "chatbot")
graph_builder.add_edge("chatbot", END)
```
## Adding memory
```python
# Handles memory storage within our graph
from langgraph.checkpoint.memory import MemorySaver

# Modify the graph with memory checkpoint
memory = MemorySaver()

# Compile the graph passing in memory
graph = graph_builder.compile(checkpointer=memory)

# Set up a streamming function for a sindle user
def stream_memory_responses(user_input: str):
	# unique thread ai let's maintain message history
	config = {"configurable": {"thread_id": "single_session_memory"}}

	# add defined configuration
	for event in graph.stream({"messages": [("user", user_input)]}, config):
		for value in event.values():
			if "messages" in value and value["messages"]:
				print("Agent": value["messages"])	

stream_memory_responses("What is the Colosseum?")
stream_memory_responses("Who built it?")
```
## Building a multi-tool workflow
```python
from langgraph.graph import MessagesState, START, END

# Stopping function that decides whether to call tools or end the conversation if there's no tool call in the last message.
def should_continue(state: MessageState):
	last_message = state["messages"][-1]

	# Check if the last message has tool calls.
	if last_message.tool_calls:
		return "tools"

	# End conversation if no tool calls are present
	return END

# Dynamic tool caller that returns a tool response if a tool call is present, or invokes the LLM with the chatbot node if there are no tool calls.
# Will be used as the chatbot
def call_model(state: MessagesState):
	last_message = state["messages"][-1]

	# If the last message has tool calls, return the tool's response
	if isinstance(last_message, AIMessage) and last_message.tool_calls:
		return {
			"messages": [
				AIMessage(
					content=last_message.tool_calls[0]["response"]
				)
			]
		}

	# Otherwise, proceed with regular LLM response
	return {"messages": [model_with_tools.invoke(state["messages"])]}


# FULL GRAPH
# The graph state is created for us by using MessagesState
workflow = StateGraph(MessagesState)

workflow.add_node("chatbot", call_model)
workflow.add_node("tools", tool_node)


workflow.add_edge(START, "chatbot")

# Define conditions, then loop back to chatbot
workflow.add_conditional_edges("chatbot", should_continue, ["tools", END])

workflow.add_edge("tools", "chatbot")

memory = MemorySaver()

app = workflow.compile(checkpointer=memory)
```
## Streaming multiple tool outputs
```python
from langchain_core.messages import AIMessage, HumanMessage

config = {"configurable": {"thread_id": "1"}}

def multi_tool_output(query):
	"""handles queries for different tools. For each query\
	the chatbot will return a direct tool response before the LLM
	refines the output.
	"""

	inputs = {"messages": [HumanMessage(content=query)]}

	# stream_mode set to messages enable real-time output
	for msg, metadata in app.stream(inputs, config, stream_mode="messages"):
		# If message has content and is not from a human, access just the chatbot responses so we can view it's internal behavior.
		if msg.content and not isinstance(msg, HumanMessage):
			print(msg.content, end="", flush=True)
	# Separates the answers for different queries
	print("\n")


# modified version to handle follow up questions as well as mutiple tools
def user_agent_multiturn(queries):
	"""Accepts multiple queries"""
	
	for query in queries:
		# For each user and chatbot interaction, we'll print the user's query first.
		print(f"User: {query}")

		messages_contents = []

		for msg, metadata in app.stream(
			{"messages": [HumanMessage(content=query)]}, 
			config, 
			stream_mode="messages"
		):
			
			# Filter out the human messages to print agent messages
			if msg.content and not isinstance(msg, HumanMessage):
				messages.append(msg.content)

		# Stream through messages corresponding to queries, excluding metadata
		print("Agent: " + "".join(messages_content) + '\n')

queries = ["list of very relevant questions", ...]

user_agent_multiturn(queries)
```